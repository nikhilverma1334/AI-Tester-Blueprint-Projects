
import streamlit as st
import json
import time
from tools.llm_engine import generate_test_cases_logic

# Page Config
st.set_page_config(
    page_title="Nebula | AI Test Generator",
    page_icon="ğŸŒŒ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for Premium UI
st.markdown("""
<style>
    /* Main Background */
    .stApp {
        background: linear-gradient(to bottom right, #0e1117, #151922);
        color: #e0e0e0;
    }
    
    /* Headers */
    h1, h2, h3 {
        font-family: 'Inter', sans-serif;
        color: #ffffff;
    }
    
    /* Buttons */
    .stButton>button {
        background: linear-gradient(90deg, #4f46e5 0%, #7c3aed 100%);
        color: white;
        border: none;
        border-radius: 8px;
        padding: 0.6rem 1.2rem;
        font-weight: 600;
        transition: all 0.3s ease;
    }
    .stButton>button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(124, 58, 237, 0.4);
    }
    
    /* Text Area */
    .stTextArea>div>div>textarea {
        background-color: #1e293b;
        color: #ffffff;
        border: 1px solid #334155;
        border-radius: 8px;
    }
    
    /* Cards/Expanders */
    .streamlit-expanderHeader {
        background-color: #1e293b;
        border-radius: 8px;
        color: white;
    }
    
    /* Metrics */
    div[data-testid="stMetricValue"] {
        color: #7c3aed;
    }
</style>
""", unsafe_allow_html=True)

def main():
    # Header Section
    col1, col2 = st.columns([0.1, 0.9])
    with col1:
        st.markdown("# ğŸŒŒ")
    with col2:
        st.title("Nebula Test Architect")
        st.markdown("_Determinism in every generated byte._")
    
    st.divider()

    # Sidebar Configuration
    with st.sidebar:
        st.markdown("### âš™ï¸ Configuration")
        model = st.selectbox(
            "AI Model", 
            ["llama3.2", "llama3", "mistral", "deepseek-coder"], 
            index=0,
            help="Select the local Ollama model to power the generation."
        )
        
        st.markdown("---")
        st.markdown("### ğŸ“Š Status")
        st.caption(f"Target: **localhost:11434**")
        st.caption(f"Selected Model: **{model}**")
        
        st.markdown("---")
        with st.expander("â„¹ï¸ About Nebula"):
             st.markdown("""
             **Nebula** uses your local Ollama instance to generate strict, structured test cases.
             
             **Architecture:**
             - **Action**: Ollama CLI
             - **Nexus**: Python Logic
             - **Trigger**: Streamlit UI
             """)

    # Main Workspace
    with st.container():
        input_text = st.text_area(
            "Input Requirement / Code",
            height=180,
            placeholder="e.g., Generate tests for a Python login function that requires email, password, and handles SQL injection...",
            help="Paste requirements or code snippets here."
        )

        col_act, col_info = st.columns([1, 4])
        with col_act:
            generate_btn = st.button("Generate Test Suite ğŸš€", use_container_width=True)
            
        if generate_btn:
            if not input_text.strip():
                st.warning("âš ï¸ Input payload is empty. Please provide requirements.")
            else:
                run_generation(input_text, model)

def run_generation(text, model):
    # Progress visualization
    status_container = st.status("ğŸš€ Initiating B.L.A.S.T. Protocol...", expanded=True)
    
    try:
        status_container.write("âš¡ Nexus Layer: Analyzing Input Pattern...")
        time.sleep(0.5) # UI Micro-interaction
        
        status_container.write(f"ğŸ§  Action Layer: Querying {model}...")
        
        # Call the logic
        raw_response = generate_test_cases_logic(text, model_name=model)
        
        # Parse Logic
        try:
            # Check for known error JSONs first
            data = None
            if raw_response.strip().startswith("{") and raw_response.strip().endswith("}"):
                 data = json.loads(raw_response)
            
            # Handle Application Errors (Ollama down/Model missing)
            if data and "error" in data:
                status_container.update(label="âŒ Action Layer Failed", state="error", expanded=True)
                st.error(f"**System Error:** {data['error']}")
                return

            # Clean Parsing
            if not data:
                # If strict JSON failed, try fuzzy finding again (Nexus Layer Logic in UI for safety)
                json_start = raw_response.find('{')
                json_end = raw_response.rfind('}') + 1
                if json_start != -1 and json_end != -1:
                    json_str = raw_response[json_start:json_end]
                    data = json.loads(json_str)
            
            if data:
                status_container.update(label="âœ… Payload Delivered", state="complete", expanded=False)
                render_results(data)
            else:
                status_container.update(label="âš ï¸ Parse Warning", state="error")
                st.error("The model response was not valid JSON. Here is the raw output:")
                st.code(raw_response, language="markdown")
                
        except json.JSONDecodeError:
            status_container.update(label="âš ï¸ Valid JSON Not Found", state="error")
            st.error("Could not parse the model response.")
            with st.expander("Debug Raw Output"):
                st.code(raw_response)
                
    except Exception as e:
        status_container.update(label="âŒ Critical Failure", state="error")
        st.error(f"An unexpected error occurred: {str(e)}")

def render_results(data):
    """Refined UI for displaying test cases"""
    
    # Summary Metrics
    st.markdown("### ğŸ“‹ Test Suite Summary")
    
    # Safely get summary with defaults
    summary = data.get('test_suite_summary', {})
    feature_name = summary.get('feature', 'Untitled Feature')
    total_count = summary.get('total_tests', 0)
    
    m1, m2, m3 = st.columns(3)
    m1.metric("Feature", feature_name[:20]+"..." if len(feature_name)>20 else feature_name)
    m2.metric("Total Tests", total_count)
    m3.metric("Format", "JSON / BDD")
    
    st.markdown("---")
    
    # Tab layout
    tab_view, tab_json = st.tabs(["ğŸ‘ï¸ Visual View", "ğŸ’¾ JSON Payload"])
    
    with tab_view:
        for tc in data.get("test_cases", []):
            priority_color = "ğŸ”´" if tc.get('priority') == "P0" else "mj" if tc.get('priority') == "P1" else "ğŸŸ¢"
            
            with st.expander(f"{priority_color} {tc.get('id')} : {tc.get('title')}"):
                c1, c2 = st.columns([1, 1])
                with c1:
                    # Type Badges
                    type_str = tc.get('type', 'Functional')
                    type_icon = "âœ…" if "Positive" in type_str else "ğŸ”¸" if "Negative" in type_str else "ğŸ›¡ï¸" if "Security" in type_str else "âš¡"
                    st.markdown(f"**Type:** {type_icon} `{type_str}`")
                    
                    st.markdown("**Preconditions:**")
                    for p in tc.get("preconditions", []):
                        st.markdown(f"- {p}")
                with c2:
                    st.markdown("**Expected Result:**")
                    st.info(tc.get("expected_result"))
                
                st.markdown("**Test Steps:**")
                steps_md = ""
                for step in tc.get("steps", []):
                    steps_md += f"1. {step}\n"
                st.markdown(steps_md)
    
    with tab_json:
        st.caption("Copy this payload for your automation framework.")
        st.code(json.dumps(data, indent=2), language="json")

if __name__ == "__main__":
    main()
