# ğŸŒŒ Nebula Test Architect

**Nebula** is a local, AI-powered test case generation tool. It leverages **Ollama** and **Large Language Models (LLMs)** to convert natural language requirements or code snippets into structured, professional test suitesâ€”keeping 100% of your data on your local machine.

---

## ğŸ—ï¸ Architecture

The system follows the **A.N.T. (Action, Nexus, Trigger)** architecture to ensure deterministic outputs from probabilistic models.

```mermaid
graph TD
    User([ğŸ‘¤ User]) -->|Input Requirement| UI[Trigger Layer: Streamlit UI]
    UI -->|Payload| Nexus[Nexus Layer: llm_engine.py]
    
    subgraph Local Environment
        Nexus -->|Prompt + JSON Schema| Ollama[Action Layer: Ollama API]
        Ollama -->|"LLM Inference (Llama 3.2)"| Nexus
    end
    
    Nexus -->|Validate & Clean JSON| UI
    UI -->|Render Badges & Tables| User
```

### Components
1.  **Trigger Layer (`app.py`)**: A modern **Streamlit** dashboard that handles user input and visualizes results with badges (âœ… Positive, ğŸ”¸ Negative).
2.  **Nexus Layer (`tools/llm_engine.py`)**: The application logic that constructs strict prompts, enforces JSON schemas, and handles errors (e.g., if Ollama is down).
3.  **Action Layer (Ollama)**: The local AI server running models like `llama3.2` to generate the actual content.

---

## ğŸš€ Getting Started

### Prerequisites
1.  **Python 3.8+** installed.
2.  **Ollama** installed and running. [Download Ollama](https://ollama.com)
3.  **Model Pulled**:
    ```bash
    ollama pull llama3.2
    ```

### Installation

1.  Clone the repository.
2.  Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```

### Running the App

1.  Ensure Ollama is running (`ollama serve` or via Desktop app).
2.  Launch Nebula:
    ```bash
    streamlit run app.py
    ```
3.  Open your browser to `http://localhost:8501`.

---

## ğŸ“‚ Project Structure

```text
AItesterBlueprint/
â”œâ”€â”€ app.py                  # Main Streamlit Application (Frontend)
â”œâ”€â”€ requirements.txt        # Python Dependencies
â”œâ”€â”€ architecture/           # SOPs and Design Docs
â”‚   â””â”€â”€ test_generation_sop.md
â”œâ”€â”€ tools/                  # Core Logic Scripts
â”‚   â””â”€â”€ llm_engine.py       # Interacts with Ollama & Parses JSON
â””â”€â”€ README.md               # Project Documentation
```

## ğŸ›¡ï¸ Features

-   **Zero Data Leakage**: All processing happens on localhost.
-   **Structured JSON**: Outputs are ready for automation frameworks.
-   **Smart Categorization**: Automatically tags tests as âœ… Positive, ğŸ”¸ Negative, or ğŸ›¡ï¸ Security.
-   **Multi-Model Support**: Switch between Llama 3, Mistral, or DeepSeek from the sidebar.
